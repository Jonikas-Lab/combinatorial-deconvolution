#! /usr/bin/env python

"""
Basic functions for combinatorial deconvolution of mutant pools, i.e. matching deepseq ___________
 -- Weronika Patena, 2013
"""

# standard library
from __future__ import division
import sys
import unittest
# other packages
# my modules
import binary_code_utilities
import mutant_analysis_classes

class DeconvolutionError(Exception):
    """ Exceptions in the binary_code_utilities module."""
    pass


def readcounts_to_codewords(joint_dataset, one_cutoff=None, cutoff_per_dataset=None, cutoff_per_mutant=None):
    """ Given a reads-per-mutant dataset and cutoffs, return mutant:sample_name:0/1 dict giving readcounts below/above cutoff.

    Joint_dataset should be a mutant_analysis_classes.Insertional_mutant_pool_dataset instance, multi-dataset.

    There are three ways of specifying readcount cutoffs, and exactly one should be provided:
        - one_cutoff: a single number that will be used for all sample/mutant combinations
        - cutoff_per_dataset: a dataset_name:number dict, so the cutoff depends on the dataset
        - cutoff_per_mutant: a mutant_position:number dict, so the cutoff depends on the mutant
    
    The output is a mutant_position:dataset_name:X nested dictionary, with X 0 if the mutant had <cutoff reads in the dataset, 
     and 1 if it had >=cutoff reads.
    """
    if sum(x is not None for x in (one_cutoff, cutoff_per_dataset, cutoff_per_mutant)) != 1:
        raise DeconvolutionError("Must provide exactly one of the cutoff arguments!")
    if one_cutoff is not None:          _get_cutoff = lambda dataset_name, mutant_position: one_cutoff
    if cutoff_per_dataset is not None:  _get_cutoff = lambda dataset_name, mutant_position: cutoff_per_dataset[dataset_name]
    if cutoff_per_mutant is not None:   _get_cutoff = lambda dataset_name, mutant_position: cutoff_per_mutant[mutant_position]
    # TODO there should also be an option with the cutoff depending on BOTH dataset and mutant, somehow... Normalize the per-dataset readcounts before applying the per-mutant cutoff, or something?
    mutant_presence_dict = {}
    for mutant in joint_dataset:
        mutant_presence_dict[mutant.position] = {}
        for dataset_name,dataset_mutant_data in mutant.by_dataset.items():
            if_present = int(bool(dataset_mutant_data.total_read_count >= _get_cutoff(dataset_name, mutant.position)))
            mutant_presence_dict[mutant.position][dataset_name] = if_present
    return mutant_presence_dict


def read_codewords_from_file(infile_name, new_sample_names=None):
    """ Read sample codewords used for combinatorial pooling, from file generated by robotic_plate_transfer.write_data_to_outfile.

    (robotic_plate_transfer is in the ../../combinatorial_pooling/code folder.)
    Only reads the first "sample_number plate_and_well_position codeword transfers volume" table in the infile, ignores the rest.

    Returns sample_number:codeword dictionary.

    If new_sample_names isn't None, it should be an old_name:new_name dictionary - the new names will then be used in the output.
    """
    if new_sample_names is not None:
        if len(set(new_sample_names.values())) != len(new_sample_names):
            raise DeconvolutionError("All values in the new_sample_names dict must be unique!")
    sample_to_codeword = {}
    inside_relevant_section = False
    for line in open(infile_name):
        # skip lines until inside the section we want to parse
        if not inside_relevant_section:
            if line.startswith('sample_number\tplate_and_well_position\tcodeword'):
                inside_relevant_section=True
                continue
        # inside the section we want to parse:
        if inside_relevant_section:
            # an empty line or another header means the end of the section - just finish the loop.
            if (not line.strip()) or line.startswith('sample_number'):
                break
            # for each line in the section, just grab sample name and codeword from each line and put in the data dict
            fields = line.strip('\n').split('\t')
            try:                sample_name, codeword = fields[0], fields[2]
            except IndexError:  raise DeconvolutionError("Cannot parse line! \"%s\""%line)
            # optionally convert original to new sample names
            if new_sample_names is not None:
                sample_name = new_sample_names[sample_name]
            sample_to_codeword[sample_name] = codeword
    return sample_to_codeword


def find_closest_codewords(____):
    """ ___ """
    pass
    # useful stuff: binary_code_utilities.Hamming_distance, binary_code_utilities.bit_change_count (separate 0->1 and 1->0)
    # TODO implement!
    # TODO unit-test!


###################################################### Testing ###########################################################

class Testing(unittest.TestCase):
    """ Runs unit-tests for this module. """

    def test__readcounts_to_codewords(self):
        # convenience function for easier output checking:
        def _convert_output(output, datasets, mutants):
            """ Given mutant:datasets:0/1 output dict and lists of datasets and mutants in order, return string like '01 11'. """
            mutant_codewords = {mutant.position: ''.join(str(output[mutant.position][dataset]) for dataset in datasets) 
                                for mutant in mutants}
            return ' '.join([mutant_codewords[mutant.position] for mutant in mutants])
        # make a test case with 3 dataset and 3 mutants: readcounts 0,1,10; 9,20,0; 1,0,3
        datasets = ['A', 'B', 'C']
        pos1 = mutant_analysis_classes.Insertion_position('chr1', '+', position_before=100, immutable=True)
        pos2 = mutant_analysis_classes.Insertion_position('chr2', '-', position_after=501, immutable=True)
        pos3 = mutant_analysis_classes.Insertion_position('chr3', '+', position_before=200, position_after=201, immutable=True)
        mutant1 = mutant_analysis_classes.Insertional_mutant(pos1, multi_dataset=True)
        mutant2 = mutant_analysis_classes.Insertional_mutant(pos2, multi_dataset=True)
        mutant3 = mutant_analysis_classes.Insertional_mutant(pos3, multi_dataset=True)
        mutants = [mutant1, mutant2, mutant3]
        # the three numerical arguments to add_counts are total_reads,perfect_reads,sequence_variants - only the first matters.
        mutant1.add_counts(0, 0, 0, dataset_name=datasets[0])
        mutant1.add_counts(1, 1, 1, dataset_name=datasets[1])
        mutant1.add_counts(10, 10, 1, dataset_name=datasets[2])
        mutant2.add_counts(9, 9, 1, dataset_name=datasets[0])
        mutant2.add_counts(20, 20, 1, dataset_name=datasets[1])
        mutant2.add_counts(0, 0, 0, dataset_name=datasets[2])
        mutant3.add_counts(1, 1, 1, dataset_name=datasets[0])
        mutant3.add_counts(0, 0, 0, dataset_name=datasets[1])
        mutant3.add_counts(3, 3, 1, dataset_name=datasets[2])
        dataset = mutant_analysis_classes.Insertional_mutant_pool_dataset(multi_dataset=True)
        for mutant in mutants:  dataset.add_mutant(mutant)
        # single cutoff (checking all relevant values)
        #   3 mutants: readcounts 0,1,10; 9,20,0; 1,0,3
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=0), datasets, mutants)  == '111 111 111'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=1), datasets, mutants)  == '011 110 101'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=2), datasets, mutants)  == '001 110 001'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=3), datasets, mutants)  == '001 110 001'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=4), datasets, mutants)  == '001 110 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=9), datasets, mutants)  == '001 110 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=10), datasets, mutants) == '001 010 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=11), datasets, mutants) == '000 010 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=20), datasets, mutants) == '000 010 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=21), datasets, mutants) == '000 000 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=99), datasets, mutants) == '000 000 000'
        # per-dataset cutoffs
        #   3 mutants: readcounts 0,1,10; 9,20,0; 1,0,3
        _output = lambda CpD: _convert_output(readcounts_to_codewords(dataset, cutoff_per_dataset=CpD), datasets, mutants)
        assert _output({'A':1, 'B':1, 'C':1}) == '011 110 101'
        assert _output({'A':3, 'B':10, 'C':1}) == '001 110 001'
        assert _output({'A':2, 'B':10, 'C':4}) == '001 110 000'
        # per-mutant cutoffs
        #   3 mutants: readcounts 0,1,10; 9,20,0; 1,0,3
        _output = lambda CpM: _convert_output(readcounts_to_codewords(dataset, cutoff_per_mutant=CpM), datasets, mutants)
        assert _output({pos1:1, pos2:1, pos3:1}) ==  '011 110 101'
        assert _output({pos1:2, pos2:2, pos3:1}) ==  '001 110 101'
        assert _output({pos1:2, pos2:2, pos3:4}) ==  '001 110 000'
        assert _output({pos1:2, pos2:10, pos3:4}) == '001 010 000'
        # make sure that it only works with exactly one codeword argument - won't work with 0, any combination of 2, or all 3.
        O, D, M = 1, {'A':1, 'B':1, 'C':1}, {pos1:1, pos2:1, pos3:1}
        self.assertRaises(DeconvolutionError, readcounts_to_codewords, dataset)
        self.assertRaises(DeconvolutionError, readcounts_to_codewords, dataset, one_cutoff=O, cutoff_per_dataset=D)
        self.assertRaises(DeconvolutionError, readcounts_to_codewords, dataset, one_cutoff=O, cutoff_per_mutant=M)
        self.assertRaises(DeconvolutionError, readcounts_to_codewords, dataset, cutoff_per_dataset=D, cutoff_per_mutant=M)
        self.assertRaises(DeconvolutionError, readcounts_to_codewords, dataset, one_cutoff=O, cutoff_per_dataset=D, cutoff_per_mutant=M)

    def test__read_codewords_from_file(self):
        # basic file
        infile1 = 'test_data/INPUT_codewords_1.txt'
        output1 = read_codewords_from_file(infile1)
        assert output1 == {'0':'001', '1':'010', '2':'011', '3':'100', '4':'101', '5':'110', '6':'111'}
        # a file with another code, and with a mirror-codeword section too
        infile2 = 'test_data/INPUT_codewords_2.txt'
        output2 = read_codewords_from_file(infile2)
        assert output2 == {'0':'0011', '1':'0101', '2':'0110', '3':'1001', '4':'1010', '5':'1100', '6':'1111'}
        # trying out the new_sample_names optional dict
        output1b = read_codewords_from_file(infile1, {str(x):x+10 for x in range(7)})
        assert output1b == {10:'001', 11:'010', 12:'011', 13:'100', 14:'101', 15:'110', 16:'111'}
        output2b = read_codewords_from_file(infile2, {'0':'dA', '1':'dB', '2':'dC', '3':'dD', '4':'dE', '5':'dF', '6':'dG'})
        assert output2b == {'dA':'0011', 'dB':'0101', 'dC':'0110', 'dD':'1001', 'dE':'1010', 'dF':'1100', 'dG':'1111'}
        # fail if new_sample_names values are non-unique
        self.assertRaises(DeconvolutionError, read_codewords_from_file, infile1, {str(x):'1' for x in range(7)})
        self.assertRaises(DeconvolutionError, read_codewords_from_file, infile1, {str(x):min(x,5) for x in range(7)})
        self.assertRaises(DeconvolutionError, read_codewords_from_file, infile1, {str(x):('A' if x<3 else 'B') for x in range(7)})

    # LATER-TODO add more unit-tests!


if __name__=='__main__':
    """ If module is run directly, run tests. """
    print "This is a module for import by other programs - it doesn't do anything on its own.  Running tests..."
    unittest.main()
