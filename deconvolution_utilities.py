#! /usr/bin/env python

"""
Basic functions for combinatorial deconvolution of mutant pools, i.e. matching deepseq ___________
 -- Weronika Patena, 2013
"""

# standard library
from __future__ import division
import sys
import unittest
# other packages
# my modules
import binary_code_utilities
import mutant_analysis_classes

class DeconvolutionError(Exception):
    """ Exceptions in the binary_code_utilities module."""
    pass


def readcounts_to_codewords(joint_dataset, one_cutoff=None, cutoff_per_dataset=None, cutoff_per_mutant=None):
    """ Given a reads-per-mutant dataset and cutoffs, return mutant:sample_name:0/1 dict giving readcounts below/above cutoff.

    Joint_dataset should be a mutant_analysis_classes.Insertional_mutant_pool_dataset instance, multi-dataset.

    There are three ways of specifying readcount cutoffs, and exactly one should be provided:
        - one_cutoff: a single number that will be used for all sample/mutant combinations
        - cutoff_per_dataset: a dataset_name:number dict, so the cutoff depends on the dataset
        - cutoff_per_mutant: a mutant_position:number dict, so the cutoff depends on the mutant
    
    The output is a mutant_position:dataset_name:X nested dictionary, with X 0 if the mutant had <cutoff reads in the dataset, 
     and 1 if it had >=cutoff reads.
    """
    if sum(x is not None for x in (one_cutoff, cutoff_per_dataset, cutoff_per_mutant)) != 1:
        raise DeconvolutionError("Must provide exactly one of the cutoff arguments!")
    if one_cutoff is not None:          _get_cutoff = lambda dataset_name, mutant_position: one_cutoff
    if cutoff_per_dataset is not None:  _get_cutoff = lambda dataset_name, mutant_position: cutoff_per_dataset[dataset_name]
    if cutoff_per_mutant is not None:   _get_cutoff = lambda dataset_name, mutant_position: cutoff_per_mutant[mutant_position]
    # TODO there should also be an option with the cutoff depending on BOTH dataset and mutant, somehow... Normalize the per-dataset readcounts before applying the per-mutant cutoff, or something?
    mutant_presence_dict = {}
    for mutant in joint_dataset:
        mutant_presence_dict[mutant.position] = {}
        for dataset_name,dataset_mutant_data in mutant.by_dataset.items():
            if_present = int(bool(dataset_mutant_data.total_read_count >= _get_cutoff(dataset_name, mutant.position)))
            mutant_presence_dict[mutant.position][dataset_name] = if_present
    return mutant_presence_dict


def read_codewords_from_file(infile_name, new_sample_names=None):
    """ Read sample codewords used for combinatorial pooling, from file generated by robotic_plate_transfer.write_data_to_outfile.

    (robotic_plate_transfer is in the ../../combinatorial_pooling/code folder.)
    Only reads the first "sample_number plate_and_well_position codeword transfers volume" table in the infile, ignores the rest.

    Returns sample_number:codeword dictionary, where codewords are binary_code_utilities.Binary_codeword objects.

    If new_sample_names isn't None, it should be an old_name:new_name dictionary - the new names will then be used in the output.
    """
    if new_sample_names is not None:
        if len(set(new_sample_names.values())) != len(new_sample_names):
            raise DeconvolutionError("All values in the new_sample_names dict must be unique!")
    sample_to_codeword = {}
    inside_relevant_section = False
    for line in open(infile_name):
        # skip lines until inside the section we want to parse
        if not inside_relevant_section:
            if line.startswith('sample_number\tplate_and_well_position\tcodeword'):
                inside_relevant_section=True
                continue
        # inside the section we want to parse:
        if inside_relevant_section:
            # an empty line or another header means the end of the section - just finish the loop.
            if (not line.strip()) or line.startswith('sample_number'):
                break
            # for each line in the section, just grab sample name and codeword from each line and put in the data dict
            fields = line.strip('\n').split('\t')
            try:                sample_name, codeword_string = fields[0], fields[2]
            except IndexError:  raise DeconvolutionError("Cannot parse line! \"%s\""%line)
            # optionally convert original to new sample names
            if new_sample_names is not None:
                sample_name = new_sample_names[sample_name]
            sample_to_codeword[sample_name] = binary_code_utilities.Binary_codeword(codeword_string)
    return sample_to_codeword


def find_closest_codeword(ref_codeword, sample_to_codeword, min_distance_difference=1):
    """ Returns the sample with the closest codeword match to the ref_codeword, and the Hamming distance.

    Inputs: a 0/1 string codeword (a binary_code_utilities.Binary_codeword instance, or just a string), 
    and a sample:codeword dictionary containing more such codewords (values must be unique).

    Finds the sample(s) with codeword(s) with the lowest and second-lowest Hamming distance to the ref_codeword:
        - if there is a single sample with the lowest distance, and the difference between the lowest and second-lowest distances 
            is <=min_distance_difference, return the (closest_sample_name, min_Hamming_distance) tuple for the lowest.
        - otherwise, return a (None, min_Hamming_distance) tuple.
    """
    # useful stuff in binary_code_utilities: Binary_codeword object, Hamming_distance, bit_change_count (separate 0->1 and 1->0)
    # make sure codewords are unique
    if sample_to_codeword is not None:
        if len(set(sample_to_codeword.values())) != len(sample_to_codeword):
            raise DeconvolutionError("All values in the sample_to_codeword dict must be unique!")
    if min_distance_difference<1:
        raise DeconvolutionError("min_distance_difference must be an integer 1 or higher!")
    # LATER-TODO should probably rename Hamming_distance to be lowercase, since it's a function...
    if not isinstance(ref_codeword, binary_code_utilities.Binary_codeword):
        ref_codeword = binary_code_utilities.Binary_codeword(ref_codeword)
    # Just calculate the Hamming distance to all expected codewords
    #  MAYBE-TODO could probably optimize this a lot!  If only with caching...
    sample_to_distance = {sample:binary_code_utilities.Hamming_distance(ref_codeword, codeword) 
                                   for sample,codeword in sample_to_codeword.items()}
    min_distance = min(sample_to_distance.values())
    low_dist_samples = [sample for sample,distance in sample_to_distance.items() 
                        if distance < min_distance+min_distance_difference]
    if len(low_dist_samples)==1:    return (low_dist_samples[0], min_distance) 
    else:                           return (None, min_distance) 
    # TODO is this what I actually want to return, or something else?...  Could optionally return the full sorted distance_to_N_samples, or the top 2 of that, or something...


###################################################### Testing ###########################################################

class Testing(unittest.TestCase):
    """ Runs unit-tests for this module. """

    def test__readcounts_to_codewords(self):
        # convenience function for easier output checking:
        def _convert_output(output, datasets, mutants):
            """ Given mutant:datasets:0/1 output dict and lists of datasets and mutants in order, return string like '01 11'. """
            mutant_codewords = {mutant.position: ''.join(str(output[mutant.position][dataset]) for dataset in datasets) 
                                for mutant in mutants}
            return ' '.join([mutant_codewords[mutant.position] for mutant in mutants])
        # make a test case with 3 dataset and 3 mutants: readcounts 0,1,10; 9,20,0; 1,0,3
        datasets = ['A', 'B', 'C']
        pos1 = mutant_analysis_classes.Insertion_position('chr1', '+', position_before=100, immutable=True)
        pos2 = mutant_analysis_classes.Insertion_position('chr2', '-', position_after=501, immutable=True)
        pos3 = mutant_analysis_classes.Insertion_position('chr3', '+', position_before=200, position_after=201, immutable=True)
        mutant1 = mutant_analysis_classes.Insertional_mutant(pos1, multi_dataset=True)
        mutant2 = mutant_analysis_classes.Insertional_mutant(pos2, multi_dataset=True)
        mutant3 = mutant_analysis_classes.Insertional_mutant(pos3, multi_dataset=True)
        mutants = [mutant1, mutant2, mutant3]
        # the three numerical arguments to add_counts are total_reads,perfect_reads,sequence_variants - only the first matters.
        mutant1.add_counts(0, 0, 0, dataset_name=datasets[0])
        mutant1.add_counts(1, 1, 1, dataset_name=datasets[1])
        mutant1.add_counts(10, 10, 1, dataset_name=datasets[2])
        mutant2.add_counts(9, 9, 1, dataset_name=datasets[0])
        mutant2.add_counts(20, 20, 1, dataset_name=datasets[1])
        mutant2.add_counts(0, 0, 0, dataset_name=datasets[2])
        mutant3.add_counts(1, 1, 1, dataset_name=datasets[0])
        mutant3.add_counts(0, 0, 0, dataset_name=datasets[1])
        mutant3.add_counts(3, 3, 1, dataset_name=datasets[2])
        dataset = mutant_analysis_classes.Insertional_mutant_pool_dataset(multi_dataset=True)
        for mutant in mutants:  dataset.add_mutant(mutant)
        # single cutoff (checking all relevant values)
        #   3 mutants: readcounts 0,1,10; 9,20,0; 1,0,3
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=0), datasets, mutants)  == '111 111 111'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=1), datasets, mutants)  == '011 110 101'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=2), datasets, mutants)  == '001 110 001'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=3), datasets, mutants)  == '001 110 001'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=4), datasets, mutants)  == '001 110 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=9), datasets, mutants)  == '001 110 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=10), datasets, mutants) == '001 010 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=11), datasets, mutants) == '000 010 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=20), datasets, mutants) == '000 010 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=21), datasets, mutants) == '000 000 000'
        assert _convert_output(readcounts_to_codewords(dataset, one_cutoff=99), datasets, mutants) == '000 000 000'
        # per-dataset cutoffs
        #   3 mutants: readcounts 0,1,10; 9,20,0; 1,0,3
        _output = lambda CpD: _convert_output(readcounts_to_codewords(dataset, cutoff_per_dataset=CpD), datasets, mutants)
        assert _output({'A':1, 'B':1, 'C':1}) == '011 110 101'
        assert _output({'A':3, 'B':10, 'C':1}) == '001 110 001'
        assert _output({'A':2, 'B':10, 'C':4}) == '001 110 000'
        # per-mutant cutoffs
        #   3 mutants: readcounts 0,1,10; 9,20,0; 1,0,3
        _output = lambda CpM: _convert_output(readcounts_to_codewords(dataset, cutoff_per_mutant=CpM), datasets, mutants)
        assert _output({pos1:1, pos2:1, pos3:1}) ==  '011 110 101'
        assert _output({pos1:2, pos2:2, pos3:1}) ==  '001 110 101'
        assert _output({pos1:2, pos2:2, pos3:4}) ==  '001 110 000'
        assert _output({pos1:2, pos2:10, pos3:4}) == '001 010 000'
        # make sure that it only works with exactly one codeword argument - won't work with 0, any combination of 2, or all 3.
        O, D, M = 1, {'A':1, 'B':1, 'C':1}, {pos1:1, pos2:1, pos3:1}
        self.assertRaises(DeconvolutionError, readcounts_to_codewords, dataset)
        self.assertRaises(DeconvolutionError, readcounts_to_codewords, dataset, one_cutoff=O, cutoff_per_dataset=D)
        self.assertRaises(DeconvolutionError, readcounts_to_codewords, dataset, one_cutoff=O, cutoff_per_mutant=M)
        self.assertRaises(DeconvolutionError, readcounts_to_codewords, dataset, cutoff_per_dataset=D, cutoff_per_mutant=M)
        self.assertRaises(DeconvolutionError, readcounts_to_codewords, dataset, one_cutoff=O, cutoff_per_dataset=D, cutoff_per_mutant=M)

    def test__read_codewords_from_file(self):
        # convenience function: compare real output (includes Binary_codeword objects) to simple string representation of dict
        def _compare(output, expected_string):
            assert len(output) == len(expected_string.split(' '))
            for single_string in expected_string.split(' '):
                key,val = single_string.split(':')
                assert str(output[key]) == val
        # basic file
        infile1 = 'test_data/INPUT_codewords_1.txt'
        output1 = read_codewords_from_file(infile1)
        _compare(output1, '0:001 1:010 2:011 3:100 4:101 5:110 6:111')
        # a file with another code, and with a mirror-codeword section too
        infile2 = 'test_data/INPUT_codewords_2.txt'
        output2 = read_codewords_from_file(infile2)
        _compare(output2, '0:0011 1:0101 2:0110 3:1001 4:1010 5:1100 6:1111')
        # trying out the new_sample_names optional dict
        output1b = read_codewords_from_file(infile1, {str(x):str(x+10) for x in range(7)})
        _compare(output1b, '10:001 11:010 12:011 13:100 14:101 15:110 16:111')
        output2b = read_codewords_from_file(infile2, {'0':'dA', '1':'dB', '2':'dC', '3':'dD', '4':'dE', '5':'dF', '6':'dG'})
        _compare(output2b, 'dA:0011 dB:0101 dC:0110 dD:1001 dE:1010 dF:1100 dG:1111')
        # fail if new_sample_names values are non-unique
        self.assertRaises(DeconvolutionError, read_codewords_from_file, infile1, {str(x):'1' for x in range(7)})
        self.assertRaises(DeconvolutionError, read_codewords_from_file, infile1, {str(x):min(x,5) for x in range(7)})
        self.assertRaises(DeconvolutionError, read_codewords_from_file, infile1, {str(x):('A' if x<3 else 'B') for x in range(7)})

    def test__find_closest_codeword(self):
        sample_codewords = {x[0]:binary_code_utilities.Binary_codeword(x[2:]) for x in 'A:1100 B:1010 C:0011 D:1000'.split()}
        # diff 1 - if the top and second Hamming distance differ by 1, take the top one
        inputs_outputs_diff1 = ('1100 A 0, 1010 B 0, 0011 C 0, 1000 D 0, '
                                +'1111 None 2, 0000 D 1, 0110 None 2, 0111 C 1, 1110 None 1, 0001 C 1, 0100 A 1')
        # diff 1 - if the top and second Hamming distance differ by 1, count that as None - they must differ by at least 2
        inputs_outputs_diff2 = ('1100 None 0, 1010 None 0, 0011 C 0, 1000 None 0, '
                                +'1111 None 2, 0000 None 1, 0110 None 2, 0111 C 1, 1110 None 1, 0001 None 1, 0100 None 1')
        for diff, inputs_outputs in [(1, inputs_outputs_diff1), (2, inputs_outputs_diff2)]:
            for input_output_str in inputs_outputs.split(', '):
                input_str, sample, distance = input_output_str.split(' ')
                distance = int(distance)
                for input_val in (input_str, binary_code_utilities.Binary_codeword(input_str)):
                    out_sample, out_dist = find_closest_codeword(input_val, sample_codewords, diff)
                    if sample == 'None':    assert (out_sample is None and out_dist == distance)
                    else:                   assert (out_sample, out_dist) == (sample, distance)

    # LATER-TODO add more unit-tests!



if __name__=='__main__':
    """ If module is run directly, run tests. """
    print "This is a module for import by other programs - it doesn't do anything on its own.  Running tests..."
    unittest.main()
